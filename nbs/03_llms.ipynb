{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLMs\n",
    "\n",
    "> Client for interacting with LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp llms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import abc\n",
    "from dataclasses import dataclass, field\n",
    "from dataclasses_json import dataclass_json, config\n",
    "from typing import Any, Sequence\n",
    "import io\n",
    "import base64\n",
    "\n",
    "import openai\n",
    "import msglm\n",
    "from PIL import Image\n",
    "from fastcore import imghdr\n",
    "\n",
    "from fastagent_hacking import transforms as tx\n",
    "from fastagent_hacking import channels as cx\n",
    "from fastagent_hacking import streams as sx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.magic import register_cell_magic\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()  # Required to use run_if magic with async code.\n",
    "\n",
    "\n",
    "@register_cell_magic\n",
    "def run_if(line, cell):\n",
    "  if eval(line, globals()):\n",
    "    get_ipython().run_cell(cell)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# The basic unit of a message is either a string, image or raw bytes.\n",
    "_MsgLeafContent = str | Image.Image | bytes\n",
    "\n",
    "# A message can be a single leaf content or a sequence of leaf contents.\n",
    "MsgContent = _MsgLeafContent | Sequence[_MsgLeafContent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Utils for encoding/decoding messages.\n",
    "\n",
    "_TYPE_KEY = \"__type__\"\n",
    "\n",
    "\n",
    "def _encode(content: MsgContent) -> Any:\n",
    "  if isinstance(content, Image.Image):\n",
    "    buff = io.BytesIO()\n",
    "    content.save(buff, format='PNG')\n",
    "    return {\n",
    "        _TYPE_KEY: \"PIL.Image\",\n",
    "        \"data\": base64.b64encode(buff.getvalue()).decode()\n",
    "    }\n",
    "  elif isinstance(content, bytes):\n",
    "    return {\"__type__\": \"bytes\", \"data\": base64.b64encode(content).decode()}\n",
    "  elif isinstance(content, (list, tuple)):\n",
    "    return [_encode(item) for item in content]\n",
    "  elif isinstance(content, str):\n",
    "    return content\n",
    "\n",
    "  raise ValueError(f\"Cannot serialize {content} with type {type(content)}\")\n",
    "\n",
    "\n",
    "def _decode(content: Any) -> MsgContent:\n",
    "  if isinstance(content, dict) and _TYPE_KEY in content:\n",
    "    if content[_TYPE_KEY] == \"PIL.Image\":\n",
    "      bs = base64.b64decode(content[\"data\"])\n",
    "      return Image.open(io.BytesIO(bs))\n",
    "    elif content[_TYPE_KEY] == \"bytes\":\n",
    "      return base64.b64decode(content[\"data\"])\n",
    "  elif isinstance(content, list):\n",
    "    return [_decode(item) for item in content]\n",
    "  elif isinstance(content, str):\n",
    "    return content\n",
    "\n",
    "  raise ValueError(f\"Cannot deserialize {content} with type {type(content)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "@dataclass_json\n",
    "@dataclass(frozen=True)\n",
    "class Msg:\n",
    "  \"\"\"A message in a chat.\n",
    "\n",
    "  Attributes:\n",
    "\n",
    "    role: Dictates the purpose and perspective of the message. \n",
    "      For example, 'user', 'system' or 'assistant'.\n",
    "    content: The content of the message.\n",
    "    name: Optional. Associates the message to a named entity.\n",
    "      It doesn't have any effect on the LLM output. Defaults to empty string.\n",
    "  \"\"\"\n",
    "  role: str\n",
    "  content: MsgContent = field(metadata=config(\n",
    "      encoder=_encode,\n",
    "      decoder=_decode,\n",
    "  ))\n",
    "  name: str = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "@dataclass_json\n",
    "@dataclass(frozen=True)\n",
    "class MsgChunk:\n",
    "  role: str\n",
    "  content: MsgContent = field(metadata=config(\n",
    "      encoder=_encode,\n",
    "      decoder=_decode,\n",
    "  ))\n",
    "  end: bool\n",
    "  name: str = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Serialization/Deserialization of messages\n",
    "\n",
    "# str content\n",
    "msg = Msg(role=\"user\", content=\"Hello!\")\n",
    "test_eq(msg, Msg.from_json(msg.to_json()))\n",
    "\n",
    "# bytes content\n",
    "msg = Msg(role=\"ai\", content=b\"12345\")\n",
    "test_eq(msg, Msg.from_json(msg.to_json()))\n",
    "\n",
    "# PIL Image content\n",
    "img = Image.new(\"RGB\", (100, 100), color=1)\n",
    "msg = Msg(role=\"ai\", content=img)\n",
    "m = Msg.from_json(msg.to_json())\n",
    "test_eq(np.array(m.content), np.array(img))\n",
    "\n",
    "# list content\n",
    "msg = Msg(role=\"ai\", content=[\"Hello\", b\"12345\"])\n",
    "test_eq(msg, Msg.from_json(msg.to_json()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Serialization/Deserialization of message chunks\n",
    "\n",
    "msg = MsgChunk(\n",
    "    role=\"user\",\n",
    "    content=[\"Hello!\", b\"12345676\"],\n",
    "    end=True,\n",
    "    name=\"ai\",\n",
    ")\n",
    "test_eq(msg, MsgChunk.from_json(msg.to_json()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "MsgLike = Msg | MsgContent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "class Backend(abc.ABC):\n",
    "\n",
    "  @abc.abstractmethod\n",
    "  async def chat(\n",
    "      self,\n",
    "      msgs: Sequence[MsgLike],\n",
    "      *,\n",
    "      name: str = \"\",\n",
    "      temperature: float | None = None,\n",
    "      sink=None,\n",
    "  ) -> Msg:\n",
    "    \"\"\"Returns a chat response given a sequence of messages.\n",
    "    \n",
    "    Note: This method is stateless. It means that you must always\n",
    "      provide the full chat history.\n",
    "    \n",
    "    Args:\n",
    "      msgs: A sequence of messages. If a message is a `MsgContent`,\n",
    "        the 'user' equivalent role will be assumed.\n",
    "      name: Optional name to the chat assistant.\n",
    "        It doesn't have any effect on the LLM output. Defaults to empty string.\n",
    "      temperature: Optional. The temperature of the response.\n",
    "        If None, the backend will use its default value.\n",
    "      sink: Internal use only. Defaults to None.\n",
    "    \"\"\"\n",
    "\n",
    "  # TODO: Add emebd method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "class OpenaiAPI(Backend):\n",
    "\n",
    "  def __init__(self, *, model: str, api_key: str | None = None):\n",
    "    self._client = openai.AsyncOpenAI(api_key=api_key)\n",
    "    self._model = model\n",
    "\n",
    "  @tx.tfn\n",
    "  async def chat(\n",
    "      self,\n",
    "      msgs: Sequence[MsgLike],\n",
    "      *,\n",
    "      name: str = \"\",\n",
    "      temperature: float | None = None,\n",
    "      sink=None,\n",
    "  ) -> Msg:\n",
    "    stream = await self._client.chat.completions.create(\n",
    "        messages=[self._to_openai_msg(msg) for msg in msgs],\n",
    "        model=self._model,\n",
    "        temperature=temperature,\n",
    "        stream=True,\n",
    "    )\n",
    "    content = \"\"\n",
    "    async for chunk in stream:\n",
    "      [choice] = chunk.choices\n",
    "      delta = choice.delta.content or \"\"\n",
    "      end = choice.finish_reason is not None\n",
    "      content += delta\n",
    "      if sink:\n",
    "        await sink.put(\n",
    "            MsgChunk(\n",
    "                role=\"assistant\",\n",
    "                content=delta,\n",
    "                end=end,\n",
    "                name=name,\n",
    "            ))\n",
    "    return Msg(role=\"assistant\", content=content, name=name)\n",
    "\n",
    "  def _to_openai_msg(self, msg: Msg | MsgContent) -> dict:\n",
    "    data = msg.content if isinstance(msg, Msg) else msg\n",
    "    if isinstance(data, _MsgLeafContent):\n",
    "      data = [data]\n",
    "\n",
    "    chunks = []\n",
    "    for d in data:\n",
    "      if isinstance(d, str):\n",
    "        chunks.append(d)\n",
    "      elif isinstance(d, Image.Image):\n",
    "        buff = io.BytesIO()\n",
    "        d.save(buff, format=\"PNG\")\n",
    "        chunks.append(buff.getvalue())\n",
    "      elif isinstance(d, bytes) and bool(imghdr.what(None, d)):\n",
    "        chunks.append(d)\n",
    "      else:\n",
    "        raise ValueError(f\"Invalid message content: {d}\")\n",
    "\n",
    "    role = msg.role if isinstance(msg, Msg) else \"user\"\n",
    "\n",
    "    return msglm.mk_msg(chunks, role=role, api=\"openai\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dotenv\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%run_if os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "llm = OpenaiAPI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Msg(role='assistant', content='Hi Achraf! How can I assist you today?', name='')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%run_if os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "ai_msg = await llm.chat([\"Hi my name is Achraf\"])\n",
    "ai_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Msg(role='assistant', content='Your name is Achraf. How can I help you today, Achraf?', name='')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%run_if os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "# The `chat` API is stateless, we need to provide the full chat history.\n",
    "ai_msg = await llm.chat([\n",
    "  \"Hi my name is Achraf\", \n",
    "  ai_msg, \n",
    "  \"what's my name?\",\n",
    "])\n",
    "ai_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Msg(role='assistant', content='The image features a toucan, known for its vibrant and distinctive beak. The bird has a black body with a bright yellow throat and colorful beak, showcasing various shades of green, red, and orange. The background appears to be blurred, emphasizing the toucanâ€™s vivid colors and details.', name='')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%run_if os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "# `chat` API can handle byte images.\n",
    "img_url = \"https://www.atshq.org/wp-content/uploads/2022/07/shutterstock_1626122512.jpg\"\n",
    "img = httpx.get(img_url).content\n",
    "\n",
    "ai_msg = await llm.chat([\"What do you see in the following image?\", img])\n",
    "ai_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Msg(role='assistant', content=\"The image depicts a toucan, characterized by its distinctive large, colorful beak. The bird has a black body, bright yellow throat, and vibrant green, orange, and red hues on its beak. The background appears to be blurred, enhancing the focus on the toucan's striking features.\", name='')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%run_if os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "# `chat` API can handle PIL images.\n",
    "pil_img = Image.open(io.BytesIO(img))\n",
    "\n",
    "ai_msg = await llm.chat([\"What do you see in the following image?\", img])\n",
    "ai_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In circuits spun where thoughts align,  \n",
      "A spark of code, a thread divine.  \n",
      "With whispers soft of ones and zeroes,  \n",
      "AI emerges, our digital hero.  \n",
      "\n",
      "It learns and grows, a mind anew,  \n",
      "In data's dance, it finds its view.  \n",
      "From art to words, it weaves its tale,  \n",
      "A partner bright, where dreams set sail.  \n",
      "\n",
      "Yet in its glow, we ponder deep,  \n",
      "What truths we share, what bounds we keep.  \n",
      "For in this age of silicon dreams,  \n",
      "We navigate the light and shadows' beams.  "
     ]
    }
   ],
   "source": [
    "%%run_if os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "# `chat` can be streamed.\n",
    "async for chunk in llm.chat.stream(\n",
    "    [\"Generate a short poem about about AI\"],\n",
    "    temperature=0.7,\n",
    "):\n",
    "  print(chunk.content, end=\"\", flush=True)\n",
    "\n",
    "assert chunk.end, \"Last chunk should be the end of the response.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "class Chat(tx.Transform[MsgLike, MsgChunk]):\n",
    "\n",
    "  def __init__(\n",
    "      self,\n",
    "      backend: Backend,\n",
    "      history: Sequence[MsgLike] = [],  # TODO: Add possibility to load from DB.\n",
    "      name: str = \"\",\n",
    "  ):\n",
    "    # TODO: Add configuration for the temperature.\n",
    "    # TODO: Add possibility to send full Msg not just chunks.\n",
    "    self._backend = backend\n",
    "    self._history = list(history)\n",
    "    self._name = name\n",
    "\n",
    "  def __call__(self, chan: cx.Channel[MsgLike]) -> cx.Channel[MsgChunk]:\n",
    "    p = tx.Latch() | self.chat\n",
    "    return p(chan)\n",
    "\n",
    "  async def chat(self, msg: MsgLike):\n",
    "    if not isinstance(msg, Msg):\n",
    "      msg = Msg(role=\"user\", content=msg)\n",
    "\n",
    "    resp = \"\"\n",
    "    async for chunk in self._backend.chat.stream(\n",
    "        self._history + [msg],\n",
    "        name=self._name,\n",
    "    ):\n",
    "      resp = self._merge_content(new=chunk.content, prev=resp)\n",
    "      yield chunk\n",
    "\n",
    "    # Only record the history if the chat completion ends because\n",
    "    # chats can be interrupted mid turns.\n",
    "    self._history.extend((\n",
    "        msg,\n",
    "        Msg(role=\"assistant\", content=resp, name=self._name),\n",
    "    ))\n",
    "\n",
    "  def _merge_content(\n",
    "      self,\n",
    "      *,\n",
    "      new: MsgContent,\n",
    "      prev: MsgContent,\n",
    "  ) -> MsgContent:\n",
    "    assert isinstance(new, type(prev)), f\"Cannot merge {new} with {prev}\"\n",
    "    assert isinstance(\n",
    "        new, (str, bytes)), f\"Cannot merge {prev} with type {type(prev)}\"\n",
    "    return prev + new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code flows like a stream,  \n",
      "Indentations guide the flow,  \n",
      "Logic weaves through lines."
     ]
    }
   ],
   "source": [
    "%%run_if os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "chat = Chat(\n",
    "    llm,\n",
    "    history=[\n",
    "        \"Hi my name is Achraf\",\n",
    "        Msg(role=\"assistant\", content=\"Nice to meet you!\"),\n",
    "    ],\n",
    "    name=\"ai\",\n",
    ")\n",
    "\n",
    "\n",
    "s = sx.of(\n",
    "  cx.Packet(payload=\"Write a haiku about Physics.\", packet_type=cx.PacketType.DATA),\n",
    "  cx.Packet(payload=\"Write a haiku about Python\", packet_type=cx.PacketType.DATA),\n",
    ")\n",
    "ch = cx.as_chan(s)\n",
    "\n",
    "async for p in chat(ch):\n",
    "  if p.packet_type != cx.PacketType.DATA:\n",
    "    continue\n",
    "  chunk = p.payload\n",
    "  print(chunk.content, end=\"\", flush=True)\n",
    "\n",
    "# The history should contain 4 messages.\n",
    "#   - 2 seeded messages: [\"Hi my name is Achraf\", \"Nice to meet you!\"]\n",
    "#   - [\"Write a haiku about Physics\", <answer>] are dropped because this turn is interrupted\n",
    "#      by the next message.\n",
    "#   - [\"Write a haiku about Python\", <answer>] is the last turn.\n",
    "test_eq(len(chat._history), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
