"""Client for interacting with LLMs"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/03_llms.ipynb.

# %% auto 0
__all__ = ['MsgContent', 'MsgLike', 'Msg', 'MsgChunk', 'Backend', 'OpenaiAPI', 'Chat']

# %% ../nbs/03_llms.ipynb 3
import abc
from dataclasses import dataclass, field
from dataclasses_json import dataclass_json, config
from typing import Any, Sequence
import io
import base64

import openai
import msglm
from PIL import Image
from fastcore import imghdr

from . import transforms as tx
from . import channels as cx
from . import streams as sx

# %% ../nbs/03_llms.ipynb 8
_MsgLeafContent = str | Image.Image | bytes
MsgContent = _MsgLeafContent | Sequence[_MsgLeafContent]

_TYPE_KEY = "__type__"


def _encode_content(content: MsgContent) -> Any:
    if isinstance(content, Image.Image):
        buff = io.BytesIO()
        content.save(buff, format="PNG")
        return {
            _TYPE_KEY: "PIL.Image",
            "data": base64.b64encode(buff.getvalue()).decode(),
        }
    elif isinstance(content, bytes):
        return {"__type__": "bytes", "data": base64.b64encode(content).decode()}
    elif isinstance(content, (list, tuple)):
        return [_encode_content(item) for item in content]
    elif isinstance(content, str):
        return content

    raise ValueError(f"Cannot serialize {content} with type {type(content)}")


def _decode_content(content: Any) -> MsgContent:
    if isinstance(content, dict) and _TYPE_KEY in content:
        if content[_TYPE_KEY] == "PIL.Image":
            bs = base64.b64decode(content["data"])
            return Image.open(io.BytesIO(bs))
        elif content[_TYPE_KEY] == "bytes":
            return base64.b64decode(content["data"])
    elif isinstance(content, list):
        return [_decode_content(item) for item in content]
    elif isinstance(content, str):
        return content

    raise ValueError(f"Cannot deserialize {content} with type {type(content)}")


@dataclass_json
@dataclass(frozen=True)
class Msg:
    """A message in a chat.

    Attributes:

      role: Dictates the purpose and perspective of the message.
        For example, 'user', 'system' or 'assistant'.
      content: The content of the message.
      name: Optional. Associates the message to a named entity.
        It doesn't have any effect on the LLM output. Defaults to empty string.
    """

    role: str
    content: MsgContent = field(
        metadata=config(
            encoder=_encode_content,
            decoder=_decode_content,
        )
    )
    name: str = ""

# %% ../nbs/03_llms.ipynb 9
@dataclass_json
@dataclass(frozen=True)
class MsgChunk:
    role: str
    content: MsgContent = field(
        metadata=config(
            encoder=_encode_content,
            decoder=_decode_content,
        )
    )
    end: bool
    name: str = ""

# %% ../nbs/03_llms.ipynb 13
MsgLike = Msg | MsgContent

# %% ../nbs/03_llms.ipynb 14
class Backend(abc.ABC):

    @abc.abstractmethod
    async def chat(
        self,
        msgs: Sequence[MsgLike],
        *,
        name: str = "",
        temperature: float | None = None,
        sink=None,
    ) -> Msg:
        """Returns a chat response given a sequence of messages.

        Note: This method is stateless. It means that you must always
          provide the full chat history.

        Args:
          msgs: A sequence of messages. If a message is a `MsgContent`,
            the 'user' equivalent role will be assumed.
          name: Optional name to the chat assistant.
            It doesn't have any effect on the LLM output. Defaults to empty string.
          temperature: Optional. The temperature of the response.
            If None, the backend will use its default value.
          sink: Internal use only. Defaults to None.
        """

    # TODO: Add emebd method.

# %% ../nbs/03_llms.ipynb 16
class OpenaiAPI(Backend):

    def __init__(self, *, model: str, api_key: str | None = None):
        self._client = openai.AsyncOpenAI(api_key=api_key)
        self._model = model

    @tx.tfn
    async def chat(
        self,
        msgs: Sequence[MsgLike],
        *,
        name: str = "",
        temperature: float | None = None,
        sink=None,
    ) -> Msg:
        stream = await self._client.chat.completions.create(
            messages=[self._to_openai_msg(msg) for msg in msgs],
            model=self._model,
            temperature=temperature,
            stream=True,
        )
        content = ""
        async for chunk in stream:
            [choice] = chunk.choices
            delta = choice.delta.content or ""
            end = choice.finish_reason is not None
            content += delta
            if sink:
                await sink.put(
                    MsgChunk(
                        role="assistant",
                        content=delta,
                        end=end,
                        name=name,
                    )
                )
        return Msg(role="assistant", content=content, name=name)

    def _to_openai_msg(self, msg: Msg | MsgContent) -> dict:
        data = msg.content if isinstance(msg, Msg) else msg
        if isinstance(data, _MsgLeafContent):
            data = [data]

        chunks = []
        for d in data:
            if isinstance(d, str):
                chunks.append(d)
            elif isinstance(d, Image.Image):
                buff = io.BytesIO()
                d.save(buff, format="PNG")
                chunks.append(buff.getvalue())
            elif isinstance(d, bytes) and bool(imghdr.what(None, d)):
                chunks.append(d)
            else:
                raise ValueError(f"Invalid message content: {d}")

        role = msg.role if isinstance(msg, Msg) else "user"

        return msglm.mk_msg(chunks, role=role, api="openai")

# %% ../nbs/03_llms.ipynb 26
class Chat(tx.Transform[MsgLike, MsgChunk]):

    def __init__(
        self,
        backend: Backend,
        history: Sequence[MsgLike] = [],  # TODO: Add possibility to load from DB.
        name: str = "",
    ):
        self._backend = backend
        self._history = list(history)
        self._name = name

    def __call__(self, chan: cx.Channel[MsgLike]) -> cx.Channel[MsgChunk]:
        p = tx.Latch() | self.chat
        return p(chan)

    async def chat(self, msg: MsgLike):
        if not isinstance(msg, Msg):
            msg = Msg(role="user", content=msg)

        resp = ""
        async for chunk in self._backend.chat.stream(
            self._history + [msg],
            name=self._name,
        ):
            resp = self._merge_content(new=chunk.content, prev=resp)
            yield chunk

        # Only record the history if the chat completion completes. Cshats can be interrupted.
        self._history.extend(
            (
                msg,
                Msg(role="assistant", content=resp, name=self._name),
            )
        )

    def _merge_content(
        self,
        *,
        new: MsgContent,
        prev: MsgContent,
    ) -> MsgContent:
        assert isinstance(new, type(prev)), f"Cannot merge {new} with {prev}"
        assert isinstance(
            new, (str, bytes)
        ), f"Cannot merge {prev} with type {type(prev)}"
        return prev + new
